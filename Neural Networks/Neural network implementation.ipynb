{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b9c5858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d17abbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND operation\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y = np.array([[0, 0, 0, 1]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df2dcd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def sig(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6191c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function derivation\n",
    "def derivativeSig(z):\n",
    "    return sig(z) * (1 - sig(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fc67ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# considering weights and bias b/w -1 and 1\n",
    "# no hidden layer weights\n",
    "weights = 2 * np.random.random((2, 1)) - 1\n",
    "bias = 2 * np.random.random(1) - 1\n",
    "# lr = 0.001 # learning rate 1\n",
    "# lr = 0.01 # learning rate 2\n",
    "lr = 0.1 # learning rate 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad0f4822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[5.47540089],\n",
       "        [5.47540088]]),\n",
       " array([-8.30581135]),\n",
       " array([[2.47015766e-04],\n",
       "        [5.57028035e-02],\n",
       "        [5.57028040e-02],\n",
       "        [9.33701556e-01]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for iter in range(10000):\n",
    "    # Forward propogation without any hidden layer\n",
    "    output0 = X # output of input layer\n",
    "    output = sig(np.dot(output0, weights) + bias) # output of output layer\n",
    "    \n",
    "    # Backward propogation without any hidden layer\n",
    "    # Finding the first term = Y_pred - Y_act\n",
    "    first_term = output - Y\n",
    "\n",
    "    # Finding the second term = Oj * (1 - Oj)\n",
    "    input_for_last_layer = np.dot(output0, weights) + bias\n",
    "    second_term = derivativeSig(input_for_last_layer)\n",
    "\n",
    "    first_two = first_term * second_term\n",
    "\n",
    "    # updating the weights\n",
    "    weights_changes = np.array([[0.0], [0.0]])\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(4):\n",
    "            weights_changes[i][0] += first_two[j][0] * output0[j][i]\n",
    "\n",
    "    weights = weights - lr * weights_changes\n",
    "\n",
    "    # updating the bias\n",
    "    bias_change = 0.0\n",
    "    for j in range(4):\n",
    "        bias_change += first_two[j][0] * 1\n",
    "\n",
    "    bias = bias - lr * bias_change\n",
    "\n",
    "# Ypred values\n",
    "output = sig(np.dot(X, weights) + bias)\n",
    "\n",
    "# printing all the values\n",
    "weights, bias, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f0a2dd",
   "metadata": {},
   "source": [
    "For getting better results, play around with learning rate and number of iterations. Thereby, the Y_pred will become much closer to Y_act."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c72d72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
